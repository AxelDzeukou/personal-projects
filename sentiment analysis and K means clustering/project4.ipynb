{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b3f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n"
     ]
    }
   ],
   "source": [
    "#scrape html links\n",
    "#get text with soup\n",
    "import webspidy as spidy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#check if the href is a webpage\n",
    "def check_html(link):\n",
    "    if \".html\" in link:\n",
    "          return True  \n",
    "\n",
    "    return False\n",
    "\n",
    "#return a combination of all <p> tags in a web page and a list of <token,webpage> pairs\n",
    "def combine_texts_per_page(i,page_texts):\n",
    "    page_tokens=[]\n",
    "    for text in page_texts:\n",
    "        page_tokens.extend(word_tokenize(text))\n",
    "    \n",
    "    return list(map(lambda x: [x,i],list(set(page_tokens)))),' '.join(page_tokens)\n",
    "        \n",
    "\n",
    "#list of <token,webpage> pairs\n",
    "text_page_pairs=[]\n",
    "\n",
    "#dictionary with webpage as key and combined text as value\n",
    "page_text_dic={}\n",
    "\n",
    "#for project\n",
    "soup=spidy.get('https://www.concordia.ca/ginacody.html')\n",
    "\n",
    "#to help complete the actual link to webpage\n",
    "webpage_beginning='https://www.concordia.ca'\n",
    "#/for project\n",
    "\n",
    "#for demo\n",
    "#uncomment to set soup for demo webpage'https://www.concordia.ca/campus-life.html'\n",
    "# soup=spidy.get('https://www.concordia.ca/campus-life.html')\n",
    "#/for demo\n",
    "\n",
    "\n",
    "a_lists=soup.css('a')\n",
    "print(len(list(a_lists)))\n",
    "\n",
    "#scrape only links to webpages\n",
    "for i,a_class in enumerate(a_lists):\n",
    "    try:\n",
    "        href=a_class.attr('href')\n",
    "        if check_html(href)==True:\n",
    "            if 'https' not in href:\n",
    "                html_link=webpage_beginning+href\n",
    "                \n",
    "            elif 'https' in href:\n",
    "                html_link=href\n",
    "            \n",
    "            soup=spidy.get(html_link)\n",
    "            results=combine_texts_per_page(i,soup.css(\"p\").text())\n",
    "            text_page_pairs.extend(results[0])\n",
    "            page_text_dic[i]=results[1]\n",
    "            \n",
    "    except KeyError:\n",
    "        print(\"href unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980faf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          term  page  present\n",
      "0            a     1        1\n",
      "1     Programs     1        1\n",
      "2  Application     1        1\n",
      "3     properly     1        1\n",
      "4          2V4     1        1\n",
      "page  1    2    4    5    6    7    9    10   11   12   ...  209  210  211  \\\n",
      "term                                                    ...                  \n",
      "!     0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "$     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "%     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "&     1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  0.0  1.0  0.0   \n",
      "'     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "‘     0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "’     0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
      "“     0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "”     0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "•     0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "page  213  214  215  216  217  218  219  \n",
      "term                                     \n",
      "!     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "$     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "%     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "&     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
      "'     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "‘     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "’     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
      "“     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "”     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "•     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5961 rows x 179 columns]\n"
     ]
    }
   ],
   "source": [
    "#create incidence matrix\n",
    "import pandas as pd\n",
    "\n",
    "dic_term_page = {'term':list(map(lambda x:x[0],text_page_pairs)), 'page':list(map(lambda x:x[1],text_page_pairs))}\n",
    "\n",
    "# Calling DataFrame constructor on dictionary\n",
    "df = pd.DataFrame(dic_term_page)\n",
    "\n",
    "#incidence matrix with term as index, webpage as column, and present as value in order to get vectors\n",
    "df['present']=1\n",
    "\n",
    "incidence_matrix = pd.pivot_table(df, values='present', index=['term'],columns=['page']).fillna(0)\n",
    "\n",
    "print(incidence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc26632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 0 2 5 2 2 2 2 2 2 2 2 3 2 2 2 1 2 2 2 2 2 2 0 2 5 2 2 2 2 2 2 2 2 3 2 2\n",
      " 2 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 0 2 5 2 2 2 2 2 2 2 2 3 2 2 2 1 2 2 2 2 2 2\n",
      " 0 2 5 2 2 2 2 2 2 2 2 3 2 2 2 1 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1 1 2 2 2 2 1\n",
      " 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 4 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#run kmeans clustering  for 3 and 6\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "#list of <no of clusters,labels after clustering> pairs\n",
    "k_means_set=[[3,[]],[6,[]]]\n",
    "\n",
    "X = pd.pivot_table(df, values='present', index=['page'],columns=['term']).fillna(0)\n",
    "\n",
    "#k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[0][1]=kmeans.labels_\n",
    "\n",
    "#k=6\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[1][1]=kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39178706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 3 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is 2522.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['decommissioning', 'decommissioned', 'MyConcordia', '//www.concordia.ca/content/concordia/en/it/services/myconcordia-portal/decommissioning/notice.html', 'portal', 'mistake', 'search', 'Telephone', '//www.concordia.ca/content/concordia/en/directories.html', '//www.concordia.ca/content/concordia/en/web/a-z.html', '//www.concordia.ca/content/concordia/en/ginacody/about/faculty-members.html', 'trigger', '//www.concordia.ca/content/concordia/en/ginacody/about/strategic-plan.html', 'concordia.ca/ginacody', '//www.concordia.ca/content/concordia/en/ginacody/about/awards.html', 'Learning', 'Non-Student', 'pm', 'permit', 'assistant']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 216.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['Detail', 'decision', 'Nrb', 'Program', 'programs', 'currently', 'help', 'conference', 'date', 'required', 'any', 'Download', 'based', 'completeness', 'DNE', 'allow', 'taken', 'faculty', 'dropping', 'follows']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 912.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['drug', 'basics', 'external', 'platform', 'Geoffrey', 'music', 'custodians', 'classic', 'Program', 'programs', 'help', 'Discussion', 'Transforming', 'conference', 'required', 'cocktails', 'any', 'fails', 'rising', 'Black']\n",
      "\n",
      "\n",
      "for 6 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is -20.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['Blvd', 'Publication', 'advised', 'programs', 'currently', 'date', 'required', 'any', 'computing', 'Every', 'Sem', 'some', 'verification', 'thanks', 'funding', 'supérieur', 'Commission', 'conferral', 'systems', 'specific']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 1148.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['//www.concordia.ca/content/concordia/en/ginacody/about/faculty-members.html', 'change', 'trigger', '//www.concordia.ca/content/concordia/en/ginacody/about/strategic-plan.html', 'concordia.ca/ginacody', '//www.concordia.ca/content/concordia/en/ginacody/about/awards.html', 'family', 'Learning', '15', 'appointments', 'Non-Student', 'pm', 'permit', 'registered', 'assistant', 'past', 'Data-Driven', 'again', '2023', 'Machine']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 1241.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['decommissioning', 'signed', 'page', 'decommissioned', 'April', '11', 'accounts', 'MyConcordia', '//www.concordia.ca/content/concordia/en/it/services/myconcordia-portal/decommissioning/notice.html', 'portal', 'mistake', 'search', 'People', 'Coordinator', 'Telephone', '//www.concordia.ca/content/concordia/en/directories.html', 'permission', 'department', '//www.concordia.ca/content/concordia/en/web/a-z.html', '//www.concordia.ca/content/shared/en/news/stories/2022/11/29/smart-inverters-vulnerability-to-cyberattacks-needs-to-be-identified-and-countered-according-to-concordia-researchers.html']\n",
      "\n",
      "\n",
      "for cluster 3 the affinn_score is 912.0\n",
      "for cluster 3 the top 20 terms based on informativeness is \n",
      "['drug', 'basics', 'external', 'platform', 'Geoffrey', 'music', 'custodians', 'classic', 'Program', 'programs', 'help', 'Discussion', 'Transforming', 'conference', 'required', 'cocktails', 'any', 'fails', 'rising', 'Black']\n",
      "\n",
      "\n",
      "for cluster 4 the affinn_score is 153.0\n",
      "for cluster 4 the top 20 terms based on informativeness is \n",
      "['cumulative', 'download', 'visitors', 'Blvd', 'custodians', 'currently', 'oral', 'help', 'any', 'read', 'model', 'faculty', 'celebrated', 'diversity', 'translates', 'long', 'follows', 'develop', 'offering/gift', 'www.concordia.ca/indigenous/resources/territorial-acknowledgement.html']\n",
      "\n",
      "\n",
      "for cluster 5 the affinn_score is 216.0\n",
      "for cluster 5 the top 20 terms based on informativeness is \n",
      "['Detail', 'decision', 'Nrb', 'Program', 'programs', 'currently', 'help', 'conference', 'date', 'required', 'any', 'Download', 'based', 'completeness', 'DNE', 'allow', 'taken', 'faculty', 'dropping', 'follows']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run affinn analysis for each cluster\n",
    "from afinn import Afinn\n",
    "import math\n",
    "afinn = Afinn()\n",
    "\n",
    "#get index positions of webpages that belong to particular cluster/label to calculate afinn score\n",
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list\n",
    "\n",
    "\n",
    "#for each type of k means with different number of cluster print the afinn score for each cluster\n",
    "for k_means in k_means_set:\n",
    "    print(\"for \"+str(k_means[0])+\" clusters\"+'\\n')\n",
    "    labels_set=list(set(k_means[1]))\n",
    "\n",
    "    for label in labels_set:\n",
    "        \n",
    "        label_indices=get_index_positions(list(k_means[1]),label)\n",
    "        \n",
    "        #pages in cluster\n",
    "        clustered_pages=list(incidence_matrix.iloc[:, label_indices].columns)\n",
    "        \n",
    "        affinn_score=0\n",
    "\n",
    "        for page in clustered_pages:\n",
    "            affinn_score+=afinn.score(page_text_dic[page])\n",
    "\n",
    "        print(\"for cluster \"+str(label)+\" the affinn_score is \"+str(affinn_score))\n",
    "\n",
    "        #to measure top 20 terms based on informativeness we need to create indexer for documents in each cluster then rank\n",
    "        text_page_pairs_forCluster=list(filter(lambda x: x[1] in clustered_pages,text_page_pairs))\n",
    "        spimi_indexer={}\n",
    "\n",
    "        for token_stream in text_page_pairs_forCluster:\n",
    "            if token_stream[0] in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "\n",
    "\n",
    "            elif token_stream[0] not in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]]=[]\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "        \n",
    "        informativeness_results=list(spimi_indexer.items())\n",
    "        informativeness_results=list(map(lambda x:  x[0],sorted(informativeness_results, key=lambda x:math.log(len(clustered_pages)/len(x[1])), reverse=True)))\n",
    "        print(\"for cluster \"+str(label)+\" the top 20 terms based on informativeness is \")\n",
    "        print(informativeness_results[:20])\n",
    "        print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
