{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8560352-bf40-47ed-8756-fbb4ef61a89e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T11:52:35.689934Z",
     "iopub.status.busy": "2025-08-29T11:52:35.689934Z",
     "iopub.status.idle": "2025-08-29T11:52:59.770409Z",
     "shell.execute_reply": "2025-08-29T11:52:59.770409Z",
     "shell.execute_reply.started": "2025-08-29T11:52:35.689934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links found: 176\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# --- Helper functions ---\n",
    "def check_html(link):\n",
    "    return \".html\" in link\n",
    "\n",
    "def combine_texts_per_page(i, page_texts):\n",
    "    page_tokens = []\n",
    "    for text in page_texts:\n",
    "        page_tokens.extend(word_tokenize(text))\n",
    "    return list(map(lambda x: [x, i], list(set(page_tokens)))), ' '.join(page_tokens)\n",
    "\n",
    "\n",
    "# --- Data containers ---\n",
    "text_page_pairs = []\n",
    "page_text_dic = {}\n",
    "\n",
    "# --- Project start ---\n",
    "start_url = 'https://www.concordia.ca/ginacody.html'\n",
    "webpage_beginning = 'https://www.concordia.ca'\n",
    "\n",
    "# get soup from main page\n",
    "soup = BeautifulSoup(requests.get(start_url).text, \"html.parser\")\n",
    "a_lists = soup.find_all('a')\n",
    "print(\"Number of links found:\", len(a_lists))\n",
    "\n",
    "# scrape only .html links\n",
    "for i, a_tag in enumerate(a_lists):\n",
    "    href = a_tag.get('href')\n",
    "    if not href:\n",
    "        continue\n",
    "\n",
    "    if check_html(href):\n",
    "        if 'https' not in href:\n",
    "            html_link = webpage_beginning + href\n",
    "        else:\n",
    "            html_link = href\n",
    "\n",
    "        try:\n",
    "            page_soup = BeautifulSoup(requests.get(html_link).text, \"html.parser\")\n",
    "            results = combine_texts_per_page(i, [p.get_text() for p in page_soup.find_all(\"p\")])\n",
    "            text_page_pairs.extend(results[0])\n",
    "            page_text_dic[i] = results[1]\n",
    "        except Exception as e:\n",
    "            print(\"Error scraping\", html_link, \":\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980faf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T11:52:59.771478Z",
     "iopub.status.busy": "2025-08-29T11:52:59.771478Z",
     "iopub.status.idle": "2025-08-29T11:52:59.826929Z",
     "shell.execute_reply": "2025-08-29T11:52:59.826929Z",
     "shell.execute_reply.started": "2025-08-29T11:52:59.771478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page        1    2    4    5    6    7    9    10   11   12   ...  163  164  \\\n",
      "term                                                          ...             \n",
      "!           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0   \n",
      "$           0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "%           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "&           0.0  0.0  1.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  ...  0.0  1.0   \n",
      "'           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "étudiantes  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "étudiants   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "–           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "—           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "’           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
      "\n",
      "page        165  166  167  170  171  172  173  174  \n",
      "term                                                \n",
      "!           0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  \n",
      "$           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "%           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "&           0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  \n",
      "'           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "...         ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "étudiantes  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "étudiants   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "–           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "—           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "’           0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[5993 rows x 140 columns]\n"
     ]
    }
   ],
   "source": [
    "#create incidence matrix\n",
    "import pandas as pd\n",
    "\n",
    "dic_term_page = {'term':list(map(lambda x:x[0],text_page_pairs)), 'page':list(map(lambda x:x[1],text_page_pairs))}\n",
    "\n",
    "# Calling DataFrame constructor on dictionary\n",
    "df = pd.DataFrame(dic_term_page)\n",
    "\n",
    "#incidence matrix with term as index, webpage as column, and present as value in order to get vectors\n",
    "df['present']=1\n",
    "\n",
    "incidence_matrix = pd.pivot_table(df, values='present', index=['term'],columns=['page']).fillna(0)\n",
    "\n",
    "print(incidence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc26632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T11:52:59.826929Z",
     "iopub.status.busy": "2025-08-29T11:52:59.826929Z",
     "iopub.status.idle": "2025-08-29T11:53:01.857750Z",
     "shell.execute_reply": "2025-08-29T11:53:01.857750Z",
     "shell.execute_reply.started": "2025-08-29T11:52:59.826929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 2 2 2\n",
      " 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2\n",
      " 2 2 0 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[2 2 2 2 2 2 2 0 2 2 2 2 4 3 3 2 3 2 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 2 2 2 2\n",
      " 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 3 2 2 1 2 2 2 2 2 2 2 2\n",
      " 2 2 0 2 2 2 2 4 3 3 2 3 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2\n",
      " 3 2 2 2 2 2 2 2 2 2 3 2 2 3 2 3 2 2 2 2 2 2 2 5 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#run kmeans clustering  for 3 and 6\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "#list of <no of clusters,labels after clustering> pairs\n",
    "k_means_set=[[3,[]],[6,[]]]\n",
    "\n",
    "X = pd.pivot_table(df, values='present', index=['page'],columns=['term']).fillna(0)\n",
    "\n",
    "#k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[0][1]=kmeans.labels_\n",
    "\n",
    "#k=6\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[1][1]=kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39178706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T11:53:01.859437Z",
     "iopub.status.busy": "2025-08-29T11:53:01.858937Z",
     "iopub.status.idle": "2025-08-29T11:53:04.065495Z",
     "shell.execute_reply": "2025-08-29T11:53:04.065495Z",
     "shell.execute_reply.started": "2025-08-29T11:53:01.859437Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 3 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is 96.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['Services', 'correctly', 'notified', 'check', 'funds', 'Fellowship', 'consist', '5G', 'MASc', 'complete', '30', 'association', 'having', 'Excellence', '2W1', 'Construction', 'Catherine', 'position', 'Arts', 'reception.ginacody']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 86.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['highly', 'â\\x80\\x94', 'faculty.â\\x80\\x9d', 'but', 'outstanding', 'commitment', '17', 'application', 'matter', 'Dean', 'wide', 'that', 'enjoys', 'world', 'reputation', 'growing', '``', '50', 'innovative', 'gender']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 2456.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['mistake', \"department'sâ\\x80¯Telephone\", '300â\\x80\\x8b', '922', '321', '52â\\x80\\x8b', 'assessments', 'bike', 'â\\x80\\x8b', '171,131', '251,213', 'shuttle', 'bases', 'decarbonize', 'N.D.G', 'environmentally', 'hundred', 'placesâ\\x80\\x8b', 'LEED', '269â\\x80\\x8b']\n",
      "\n",
      "\n",
      "for 6 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is 96.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['Services', 'correctly', 'notified', 'check', 'funds', 'Fellowship', 'consist', '5G', 'MASc', 'complete', '30', 'association', 'having', 'Excellence', '2W1', 'Construction', 'Catherine', 'position', 'Arts', 'reception.ginacody']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 2.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['reimagines', 'W.', 'The', 'ka', 'highways', 'sun', 'Inside', 'why', 'De', ':', 'links', 'way', 'eligible', 'Complex', 'Officer', 'action', 'major', 'for', 'Canada', '@']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 1920.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['mistake', \"department'sâ\\x80¯Telephone\", '300â\\x80\\x8b', '922', '321', '52â\\x80\\x8b', 'assessments', 'bike', 'â\\x80\\x8b', 'demonstrates', '171,131', '251,213', 'shuttle', 'bases', 'downtown', 'decarbonize', 'N.D.G', 'environmentally', 'hundred', 'placesâ\\x80\\x8b']\n",
      "\n",
      "\n",
      "for cluster 3 the affinn_score is 94.0\n",
      "for cluster 3 the top 20 terms based on informativeness is \n",
      "['Services', 'directions', 'facilitate', 'Black', 'awards', 'curriculum', 'Office', 'Department', 'studies', 'initiative', 'renewal', 'more', 'boosts', 'grad', 'Perspectives', 'fund', 'Browse', 'stories', 'charts', 'paths']\n",
      "\n",
      "\n",
      "for cluster 4 the affinn_score is 374.0\n",
      "for cluster 4 the top 20 terms based on informativeness is \n",
      "['Services', 'entrance', 'Mbowe', 'Clubs', 'Models', 'one-stop', 'youâ\\x80\\x99re', 'representatives', 'Black', 'look', 'links', 'supplies', 'Theoretic', 'alliances', 'Machines', 'Mentors', 'journée', 'snacks', 'Any', 'stands']\n",
      "\n",
      "\n",
      "for cluster 5 the affinn_score is 152.0\n",
      "for cluster 5 the top 20 terms based on informativeness is \n",
      "['Agreement', 'By', 'level', 'importance', 'signed', 'Elders', 'links', '16', 'file', 'on-going', 'Peoples', 'KanehsatÃ¡', 'usually', 'collective', 'below', 'Settler', 'stands', '1701', 'waters', 'stakeholders']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run affinn analysis for each cluster\n",
    "from afinn import Afinn\n",
    "import math\n",
    "afinn = Afinn()\n",
    "\n",
    "#get index positions of webpages that belong to particular cluster/label to calculate afinn score\n",
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list\n",
    "\n",
    "\n",
    "#for each type of k means with different number of cluster print the afinn score for each cluster\n",
    "for k_means in k_means_set:\n",
    "    print(\"for \"+str(k_means[0])+\" clusters\"+'\\n')\n",
    "    labels_set=list(set(k_means[1]))\n",
    "\n",
    "    for label in labels_set:\n",
    "        \n",
    "        label_indices=get_index_positions(list(k_means[1]),label)\n",
    "        \n",
    "        #pages in cluster\n",
    "        clustered_pages=list(incidence_matrix.iloc[:, label_indices].columns)\n",
    "        \n",
    "        affinn_score=0\n",
    "\n",
    "        for page in clustered_pages:\n",
    "            affinn_score+=afinn.score(page_text_dic[page])\n",
    "\n",
    "        print(\"for cluster \"+str(label)+\" the affinn_score is \"+str(affinn_score))\n",
    "\n",
    "        #to measure top 20 terms based on informativeness we need to create indexer for documents in each cluster then rank\n",
    "        text_page_pairs_forCluster=list(filter(lambda x: x[1] in clustered_pages,text_page_pairs))\n",
    "        spimi_indexer={}\n",
    "\n",
    "        for token_stream in text_page_pairs_forCluster:\n",
    "            if token_stream[0] in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "\n",
    "\n",
    "            elif token_stream[0] not in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]]=[]\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "        \n",
    "        informativeness_results=list(spimi_indexer.items())\n",
    "        informativeness_results=list(map(lambda x:  x[0],sorted(informativeness_results, key=lambda x:math.log(len(clustered_pages)/len(x[1])), reverse=True)))\n",
    "        print(\"for cluster \"+str(label)+\" the top 20 terms based on informativeness is \")\n",
    "        print(informativeness_results[:20])\n",
    "        print('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
