{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b3f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n",
      "href unavailable\n"
     ]
    }
   ],
   "source": [
    "#scrape html links\n",
    "#get text with soup\n",
    "import webspidy as spidy\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#check if the href is a webpage\n",
    "def check_html(link):\n",
    "    if \".html\" in link:\n",
    "          return True  \n",
    "\n",
    "    return False\n",
    "\n",
    "#return a combination of all <p> tags in a web page and a list of <token,webpage> pairs\n",
    "def combine_texts_per_page(i,page_texts):\n",
    "    page_tokens=[]\n",
    "    for text in page_texts:\n",
    "        page_tokens.extend(word_tokenize(text))\n",
    "    \n",
    "    return list(map(lambda x: [x,i],list(set(page_tokens)))),' '.join(page_tokens)\n",
    "        \n",
    "\n",
    "#list of <token,webpage> pairs\n",
    "text_page_pairs=[]\n",
    "\n",
    "#dictionary with webpage as key and combined text as value\n",
    "page_text_dic={}\n",
    "\n",
    "#for project\n",
    "# soup=spidy.get('https://www.concordia.ca/ginacody.html')\n",
    "\n",
    "# #to help complete the actual link to webpage\n",
    "# webpage_beginning='https://www.concordia.ca'\n",
    "#/for project\n",
    "\n",
    "#for demo\n",
    "#uncomment to set soup and webpage_beginning for demo webpage'https://www.concordia.ca/campus-life.html'\n",
    "soup=spidy.get('https://www.concordia.ca/campus-life.html')\n",
    "webpage_beginning='https://www.concordia.ca'\n",
    "#/for demo\n",
    "\n",
    "\n",
    "a_lists=soup.css('a')\n",
    "print(len(list(a_lists)))\n",
    "\n",
    "#scrape only links to webpages\n",
    "for i,a_class in enumerate(a_lists):\n",
    "    try:\n",
    "        href=a_class.attr('href')\n",
    "        if check_html(href)==True:\n",
    "            if 'https' not in href:\n",
    "                html_link=webpage_beginning+href\n",
    "                \n",
    "            elif 'https' in href:\n",
    "                html_link=href\n",
    "            \n",
    "            soup=spidy.get(html_link)\n",
    "            results=combine_texts_per_page(i,soup.css(\"p\").text())\n",
    "            text_page_pairs.extend(results[0])\n",
    "            page_text_dic[i]=results[1]\n",
    "            \n",
    "    except KeyError:\n",
    "        print(\"href unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980faf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page  1    2    4    5    6    7    9    10   11   12   ...  301  302  303  \\\n",
      "term                                                    ...                  \n",
      "!     0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "#     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "$     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "%     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "&     1.0  0.0  0.0  1.0  0.0  1.0  1.0  1.0  1.0  1.0  ...  1.0  1.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "’     0.0  1.0  1.0  1.0  0.0  1.0  1.0  0.0  1.0  1.0  ...  1.0  1.0  1.0   \n",
      "“     0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "”     0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
      "•     0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "▪     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "page  305  306  307  308  309  310  311  \n",
      "term                                     \n",
      "!     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "#     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "$     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "%     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "&     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
      "...   ...  ...  ...  ...  ...  ...  ...  \n",
      "’     1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
      "“     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "”     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "•     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "▪     0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[7369 rows x 247 columns]\n"
     ]
    }
   ],
   "source": [
    "#create incidence matrix\n",
    "import pandas as pd\n",
    "\n",
    "dic_term_page = {'term':list(map(lambda x:x[0],text_page_pairs)), 'page':list(map(lambda x:x[1],text_page_pairs))}\n",
    "\n",
    "# Calling DataFrame constructor on list\n",
    "df = pd.DataFrame(dic_term_page)\n",
    "\n",
    "#incidence matrix with term as index, webpage as column, and present as value in order to get vectors\n",
    "df['present']=1\n",
    "\n",
    "incidence_matrix = pd.pivot_table(df, values='present', index=['term'],columns=['page']).fillna(0)\n",
    "\n",
    "print(incidence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc26632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[2 2 2 4 2 2 2 2 2 2 1 2 5 2 2 2 2 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 1 2 5 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 0 0\n",
      " 2 2 2 2 2 2 1 2 1 2 2 2 2 2 1 2 2 2 2 2 2 1 1 2 1 2 1 2 1 2 2 2 3 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 4 2 2 2 2 2 2 1 2 5 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 4 2 2 2 2 2 2 1 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1\n",
      " 1 2 2 2 2 1 1 2 1 2 2 2 1 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 4 2 2 2 3 5 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "#run kmeans clustering  for 3 and 6\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "#list of <no of clusters,labels after clustering> pairs\n",
    "k_means_set=[[3,[]],[6,[]]]\n",
    "\n",
    "X = pd.pivot_table(df, values='present', index=['page'],columns=['term']).fillna(0)\n",
    "\n",
    "#k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[0][1]=kmeans.labels_\n",
    "\n",
    "#k=6\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(X)\n",
    "print(kmeans.labels_)\n",
    "k_means_set[1][1]=kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39178706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 3 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is 4015.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['tomorrow', 'rise', 'cross-cutting', 'decades', 'Invest', 'span', 'objectives', 'transformational', 'poised', 'next-gen', 'Now', 'experiment', 'Next-Gen', 'realize', 'wants', 'landmark', 'spheres', 'launched', '//www.concordia.ca/content/concordia/en/campaign.html', 'brand']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 270.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['dropped', 'Register', 'receive', 'sets', 'carefully', 'help', 'C', 'availability', 'difficulties', '&', 'in', 'Departmental', 'show', 'Learning', 'Click', 'course', 'clicking', 'able', 'easily', 'search']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 1422.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['opportunity', 'career', 'THE', 'facilitated', 'wisdom', 'Webster', 'explain', 'Hall', 'puppy', 'neglected', 'event', 'exam', 'Up', 'Maison', 'help', 'exploration', 'intentionally', 'Climate', 'An', 'micro']\n",
      "\n",
      "\n",
      "for 6 clusters\n",
      "\n",
      "for cluster 0 the affinn_score is 552.0\n",
      "for cluster 0 the top 20 terms based on informativeness is \n",
      "['organisms', 'birthplace', 'BA', 'explains', 'Modern', 'journalism', 'audio-visual', 'animal', 'Contemporary', 'Simone', 'mechanisms', 'it', 'Material', \"'\", 'Admirable', 'geography', 'ensuring', '2S2', 'perform', 'powers']\n",
      "\n",
      "\n",
      "for cluster 1 the affinn_score is 470.0\n",
      "for cluster 1 the top 20 terms based on informativeness is \n",
      "['More', '2023', 'eight-week', '514-848-2888', 'Amgen', 'undertake', 'transition', 'Offering', '848-2424', 'Michael', 'abroad', 'Tuesdays', 'Longmire', 'research', 'finance', '//www.concordia.ca/content/concordia/en/students/exchanges.html', 'Mackay', 'Summer', '175', 'X']\n",
      "\n",
      "\n",
      "for cluster 2 the affinn_score is 2829.0\n",
      "for cluster 2 the top 20 terms based on informativeness is \n",
      "['tomorrow', 'rise', 'cross-cutting', 'decades', 'Invest', 'span', 'Drawing', 'objectives', 'transformational', 'poised', 'next-gen', 'Now', 'experiment', 'emerged', 'Next-Gen', 'realize', 'wants', 'landmark', 'spheres', 'launched']\n",
      "\n",
      "\n",
      "for cluster 3 the affinn_score is 164.0\n",
      "for cluster 3 the top 20 terms based on informativeness is \n",
      "['highly', 'Leadership', 'Suong', 'catalysts', 'Li', 'Lefebvre', 'l', 'Alberts', 'An', 'Climate', 'Kishk', '&', 'in', 'Sexual', '``', 'training', 'Research', 'Learning', 'Vincent', 'Thien']\n",
      "\n",
      "\n",
      "for cluster 4 the affinn_score is 270.0\n",
      "for cluster 4 the top 20 terms based on informativeness is \n",
      "['dropped', 'Register', 'receive', 'sets', 'carefully', 'help', 'C', 'availability', 'difficulties', '&', 'in', 'Departmental', 'show', 'Learning', 'Click', 'course', 'clicking', 'able', 'easily', 'search']\n",
      "\n",
      "\n",
      "for cluster 5 the affinn_score is 1422.0\n",
      "for cluster 5 the top 20 terms based on informativeness is \n",
      "['opportunity', 'career', 'THE', 'facilitated', 'wisdom', 'Webster', 'explain', 'Hall', 'puppy', 'neglected', 'event', 'exam', 'Up', 'Maison', 'help', 'exploration', 'intentionally', 'Climate', 'An', 'micro']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#run affinn analysis for each cluster\n",
    "from afinn import Afinn\n",
    "import math\n",
    "afinn = Afinn()\n",
    "\n",
    "#get index positions of webpages that belong to particular cluster/label to calculate afinn score\n",
    "def get_index_positions(list_of_elems, element):\n",
    "    ''' Returns the indexes of all occurrences of give element in\n",
    "    the list- listOfElements '''\n",
    "    index_pos_list = []\n",
    "    index_pos = 0\n",
    "    while True:\n",
    "        try:\n",
    "            # Search for item in list from indexPos to the end of list\n",
    "            index_pos = list_of_elems.index(element, index_pos)\n",
    "            # Add the index position in list\n",
    "            index_pos_list.append(index_pos)\n",
    "            index_pos += 1\n",
    "        except ValueError as e:\n",
    "            break\n",
    "    return index_pos_list\n",
    "\n",
    "\n",
    "#for each type of k means with different number of cluster print the afinn score for each cluster\n",
    "for k_means in k_means_set:\n",
    "    print(\"for \"+str(k_means[0])+\" clusters\"+'\\n')\n",
    "    labels_set=list(set(k_means[1]))\n",
    "\n",
    "    for label in labels_set:\n",
    "        \n",
    "        label_indices=get_index_positions(list(k_means[1]),label)\n",
    "        \n",
    "        #pages in cluster\n",
    "        clustered_pages=list(incidence_matrix.iloc[:, label_indices].columns)\n",
    "        \n",
    "        affinn_score=0\n",
    "\n",
    "        for page in clustered_pages:\n",
    "            affinn_score+=afinn.score(page_text_dic[page])\n",
    "\n",
    "        print(\"for cluster \"+str(label)+\" the affinn_score is \"+str(affinn_score))\n",
    "\n",
    "        #to measure top 20 terms based on informativeness we need to create indexer for documents in each cluster then rank\n",
    "        text_page_pairs_forCluster=list(filter(lambda x: x[1] in clustered_pages,text_page_pairs))\n",
    "        spimi_indexer={}\n",
    "\n",
    "        for token_stream in text_page_pairs_forCluster:\n",
    "            if token_stream[0] in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "\n",
    "\n",
    "            elif token_stream[0] not in spimi_indexer:\n",
    "                spimi_indexer[token_stream[0]]=[]\n",
    "                spimi_indexer[token_stream[0]].append(token_stream[1])\n",
    "        \n",
    "        informativeness_results=list(spimi_indexer.items())\n",
    "        informativeness_results=list(map(lambda x:  x[0],sorted(informativeness_results, key=lambda x:math.log(len(clustered_pages)/len(x[1])), reverse=True)))\n",
    "        print(\"for cluster \"+str(label)+\" the top 20 terms based on informativeness is \")\n",
    "        print(informativeness_results[:20])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d294f5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
